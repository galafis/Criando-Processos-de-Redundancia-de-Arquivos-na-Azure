Atividade proposta pela **DIO** dentro do meu programa de aprendizado em **Microsoft AI for Tech - Azure Databricks**

# Criando Processos de RedundÃ¢ncia de Arquivos no Microsoft Azure

![Imagem Hero](images/hero_image.png)

## IntroduÃ§Ã£o

Este projeto documenta a implementaÃ§Ã£o de um processo completo de redundÃ¢ncia de arquivos no Microsoft Azure utilizando o Azure Data Factory (ADF). A soluÃ§Ã£o conecta ambientes on-premises com recursos na nuvem Azure, permitindo a movimentaÃ§Ã£o segura e eficiente de dados entre esses ambientes. O foco principal Ã© criar um fluxo automatizado que extrai dados de uma tabela SQL Server on-premises, transfere-os para o Azure Data Lake Storage (ADLS) e os converte em arquivos .TXT organizados em uma estrutura de camadas (raw/bronze).

A redundÃ¢ncia de dados Ã© uma prÃ¡tica essencial para garantir a disponibilidade, a seguranÃ§a e a integridade das informaÃ§Ãµes em ambientes corporativos. Ao implementar este projeto, vocÃª estarÃ¡ criando uma soluÃ§Ã£o que nÃ£o apenas protege os dados contra perdas, mas tambÃ©m facilita sua utilizaÃ§Ã£o em diferentes contextos e aplicaÃ§Ãµes na nuvem.

## Objetivo do Projeto

O objetivo principal deste projeto Ã© estabelecer um processo automatizado e confiÃ¡vel para criar redundÃ¢ncia de dados entre ambientes on-premises e a nuvem Azure. Especificamente, buscamos:

1. Configurar a infraestrutura necessÃ¡ria no Azure Data Factory para conectar ambientes on-premises e na nuvem
2. Implementar a extraÃ§Ã£o de dados de uma tabela SQL Server local
3. Transferir esses dados para o Azure Data Lake Storage
4. Converter os dados em arquivos .TXT organizados em uma estrutura de camadas
5. Validar, publicar e executar o pipeline de dados
6. Analisar a performance e aplicar boas prÃ¡ticas

Ao final deste projeto, teremos uma soluÃ§Ã£o funcional que pode ser adaptada para diferentes cenÃ¡rios de redundÃ¢ncia de dados, contribuindo para estratÃ©gias de continuidade de negÃ³cios e recuperaÃ§Ã£o de desastres.

## PrÃ©-requisitos

Para implementar este projeto, sÃ£o necessÃ¡rios os seguintes componentes e conhecimentos:

### Recursos Azure
- Uma assinatura ativa do Microsoft Azure
- Acesso para criar e gerenciar recursos do Azure Data Factory
- PermissÃ£o para criar e configurar recursos de armazenamento (Azure Data Lake Storage Gen2)
- Capacidade de criar e gerenciar recursos de banco de dados (Azure SQL Database)

### Ambiente On-premises
- Um servidor SQL Server local com uma tabela de dados para teste
- PermissÃµes administrativas no servidor para configurar o Integration Runtime
- Conectividade de rede entre o ambiente local e o Azure

### Conhecimentos TÃ©cnicos
- Familiaridade bÃ¡sica com o portal Azure e seus serviÃ§os
- Entendimento de conceitos de bancos de dados SQL
- NoÃ§Ãµes de ETL (Extract, Transform, Load) e pipelines de dados

## Arquitetura da SoluÃ§Ã£o

A soluÃ§Ã£o implementada neste projeto segue a seguinte arquitetura:

1. **Ambiente On-premises**:
   - SQL Server com tabela de dados de origem
   - Self-hosted Integration Runtime (SHIR) para conectividade segura com o Azure

2. **Microsoft Azure**:
   - Azure Data Factory para orquestraÃ§Ã£o do fluxo de dados
   - Azure SQL Database para armazenamento opcional de dados na nuvem
   - Azure Data Lake Storage Gen2 para armazenamento dos arquivos .TXT

3. **Fluxo de Dados**:
   - ExtraÃ§Ã£o de dados da tabela SQL Server on-premises
   - TransferÃªncia para o Azure via Integration Runtime
   - Armazenamento e conversÃ£o para arquivos .TXT no Data Lake
   - OrganizaÃ§Ã£o em estrutura de camadas (raw/bronze)

Esta arquitetura proporciona um fluxo seguro e eficiente de dados entre o ambiente local e a nuvem, garantindo a redundÃ¢ncia e a disponibilidade das informaÃ§Ãµes.

## Passo a Passo da ImplementaÃ§Ã£o

### 1. ConfiguraÃ§Ã£o do Self-Hosted Integration Runtime (SHIR)

O Self-Hosted Integration Runtime Ã© um componente essencial para conectar o Azure Data Factory a recursos em redes privadas, como servidores SQL on-premises. Ele atua como uma ponte segura entre os ambientes, permitindo a movimentaÃ§Ã£o de dados sem expor diretamente os servidores locais Ã  internet.

Para configurar o SHIR:

1. **Criar o Integration Runtime no Azure Data Factory**:
   - No portal Azure, acesse sua instÃ¢ncia do Azure Data Factory
   - Navegue atÃ© "Manage" > "Integration Runtimes" > "New"
   - Selecione "Self-Hosted" como tipo de Integration Runtime
   - ForneÃ§a um nome descritivo (ex: "OnPremisesIntegrationRuntime")
   - Copie a chave de autenticaÃ§Ã£o gerada

2. **Instalar o Integration Runtime no servidor on-premises**:
   - Baixe o instalador do Integration Runtime no servidor que tem acesso ao SQL Server
   - Execute o instalador e siga o assistente de configuraÃ§Ã£o
   - Quando solicitado, insira a chave de autenticaÃ§Ã£o copiada anteriormente
   - Aguarde a conclusÃ£o do registro e a confirmaÃ§Ã£o de que o Integration Runtime estÃ¡ online

3. **Verificar a conectividade**:
   - No portal Azure, confirme que o status do Integration Runtime estÃ¡ como "Running"
   - Teste a conectividade com o SQL Server local usando a opÃ§Ã£o de diagnÃ³stico

O SHIR agora estÃ¡ configurado e pronto para facilitar a transferÃªncia segura de dados entre o ambiente on-premises e o Azure.

### 2. CriaÃ§Ã£o de Linked Services

Os Linked Services no Azure Data Factory funcionam como conexÃµes para fontes de dados e destinos. Neste projeto, precisamos configurar trÃªs Linked Services principais:

1. **Linked Service para SQL Server On-premises**:
   - No ADF Studio, navegue atÃ© "Manage" > "Linked Services" > "New"
   - Selecione "SQL Server" como tipo
   - Configure os seguintes parÃ¢metros:
     - Nome: "SqlServerOnPremises"
     - Integration Runtime: selecione o SHIR criado anteriormente
     - Nome do servidor: endereÃ§o do servidor SQL local
     - Nome do banco de dados: banco que contÃ©m a tabela de origem
     - Tipo de autenticaÃ§Ã£o: Windows ou SQL Authentication
     - Credenciais: usuÃ¡rio e senha com permissÃµes adequadas
   - Teste a conexÃ£o antes de salvar

2. **Linked Service para Azure SQL Database (opcional)**:
   - Selecione "Azure SQL Database" como tipo
   - Configure os seguintes parÃ¢metros:
     - Nome: "AzureSqlDatabase"
     - MÃ©todo de seleÃ§Ã£o: "From Azure subscription"
     - Assinatura Azure: selecione sua assinatura
     - Nome do servidor: selecione ou crie um servidor SQL Azure
     - Nome do banco de dados: selecione ou crie um banco de dados
     - Tipo de autenticaÃ§Ã£o: SQL Authentication
     - Credenciais: usuÃ¡rio e senha do banco Azure
   - Teste a conexÃ£o antes de salvar

3. **Linked Service para Azure Data Lake Storage Gen2**:
   - Selecione "Azure Data Lake Storage Gen2" como tipo
   - Configure os seguintes parÃ¢metros:
     - Nome: "AzureDataLakeStorage"
     - MÃ©todo de seleÃ§Ã£o: "From Azure subscription"
     - Assinatura Azure: selecione sua assinatura
     - Nome da conta de armazenamento: selecione ou crie uma conta ADLS Gen2
     - Teste a conexÃ£o antes de salvar

Estes Linked Services estabelecem as conexÃµes necessÃ¡rias para o fluxo de dados entre as diferentes fontes e destinos do nosso pipeline.

### 3. CriaÃ§Ã£o de Datasets

Os Datasets no Azure Data Factory representam estruturas de dados dentro das fontes de dados. Para nosso projeto, precisamos criar os seguintes datasets:

1. **Dataset para tabela SQL Server On-premises**:
   - No ADF Studio, navegue atÃ© "Author" > "+" > "Dataset"
   - Selecione "SQL Server" como tipo
   - Configure os seguintes parÃ¢metros:
     - Nome: "SqlServerTable"
     - Linked Service: selecione o Linked Service "SqlServerOnPremises"
     - Nome da tabela: selecione a tabela de origem (ex: "dbo.Customers")
   - Visualize os dados para confirmar o acesso correto

2. **Dataset para Azure Data Lake Storage (formato .TXT)**:
   - Selecione "DelimitedText" como tipo
   - Configure os seguintes parÃ¢metros:
     - Nome: "AdlsTextFile"
     - Linked Service: selecione o Linked Service "AzureDataLakeStorage"
     - Caminho do arquivo: especifique o caminho no formato "container/raw/data.txt"
     - Primeira linha como cabeÃ§alho: "True"
     - Delimitador: selecione o delimitador apropriado (ex: vÃ­rgula)
   - Defina o esquema conforme necessÃ¡rio

Estes datasets definem as estruturas de dados que serÃ£o utilizadas no pipeline de cÃ³pia, especificando tanto a origem (tabela SQL) quanto o destino (arquivo .TXT no Data Lake).

### 4. CriaÃ§Ã£o do Pipeline de CÃ³pia

O Pipeline Ã© o componente central do Azure Data Factory, orquestrando o fluxo de dados entre a origem e o destino. Para nosso projeto de redundÃ¢ncia, vamos criar um pipeline com uma atividade de cÃ³pia:

1. **Criar um novo Pipeline**:
   - No ADF Studio, navegue atÃ© "Author" > "+" > "Pipeline"
   - Nomeie o pipeline como "SqlToAdlsRedundancyPipeline"

2. **Adicionar atividade de cÃ³pia**:
   - Na caixa de ferramentas, arraste a atividade "Copy data" para o canvas
   - Nomeie a atividade como "CopySqlToAdls"

3. **Configurar a origem**:
   - Na aba "Source", selecione o dataset "SqlServerTable"
   - Se necessÃ¡rio, configure uma consulta SQL personalizada para filtrar os dados

4. **Configurar o destino**:
   - Na aba "Sink", selecione o dataset "AdlsTextFile"
   - Configure as opÃ§Ãµes de escrita:
     - OpÃ§Ã£o de cÃ³pia: "Add dynamic content" para definir o caminho dinÃ¢mico
     - Formato de nome de arquivo: inclua data/hora para versionamento (ex: "data_{yyyy-MM-dd_HH-mm}.txt")

5. **Configurar mapeamentos**:
   - Na aba "Mapping", verifique se as colunas da origem estÃ£o corretamente mapeadas para o destino
   - Ajuste tipos de dados conforme necessÃ¡rio

6. **Configurar configuraÃ§Ãµes adicionais**:
   - Na aba "Settings", configure:
     - TolerÃ¢ncia a falhas: defina comportamento em caso de erros
     - Paralelismo: ajuste conforme necessidade de performance
     - Log de atividades: habilite para monitoramento detalhado

7. **Adicionar parÃ¢metros dinÃ¢micos (opcional)**:
   - Configure parÃ¢metros de pipeline para tornar a soluÃ§Ã£o mais flexÃ­vel
   - Utilize expressÃµes para definir caminhos dinÃ¢micos baseados em data/hora

O pipeline agora estÃ¡ configurado para extrair dados da tabela SQL Server on-premises e transferi-los para o Azure Data Lake Storage como arquivos .TXT, seguindo a estrutura de camadas definida.

### 5. ValidaÃ§Ã£o, PublicaÃ§Ã£o e ExecuÃ§Ã£o

ApÃ³s configurar todos os componentes, Ã© hora de validar, publicar e executar o pipeline:

1. **Validar o pipeline**:
   - Clique no botÃ£o "Validate" para verificar se hÃ¡ erros de configuraÃ§Ã£o
   - Corrija quaisquer problemas identificados

2. **Publicar as alteraÃ§Ãµes**:
   - Clique no botÃ£o "Publish all" para publicar todos os componentes (Linked Services, Datasets, Pipeline)
   - Confirme a publicaÃ§Ã£o e aguarde a conclusÃ£o

3. **Executar o pipeline**:
   - Clique em "Add trigger" > "Trigger now" para executar o pipeline manualmente
   - Alternativamente, configure um trigger recorrente:
     - "Add trigger" > "New/Edit" > configure a programaÃ§Ã£o desejada (ex: diariamente Ã s 2h)

4. **Monitorar a execuÃ§Ã£o**:
   - Navegue atÃ© a aba "Monitor" para acompanhar o progresso da execuÃ§Ã£o
   - Verifique o status, duraÃ§Ã£o e detalhes de cada atividade
   - Em caso de falhas, analise os logs de erro para identificar e corrigir problemas

5. **Verificar os resultados**:
   - Acesse o Azure Data Lake Storage para confirmar que os arquivos .TXT foram criados corretamente
   - Verifique se a estrutura de pastas (raw/bronze) estÃ¡ conforme o esperado
   - Abra alguns arquivos para confirmar que os dados foram transferidos corretamente

A execuÃ§Ã£o bem-sucedida do pipeline confirma que o processo de redundÃ¢ncia estÃ¡ funcionando corretamente, transferindo dados do ambiente on-premises para o Azure de forma segura e organizada.

## AnÃ¡lise de Performance e Boas PrÃ¡ticas

Para garantir que o processo de redundÃ¢ncia seja eficiente e escalÃ¡vel, Ã© importante analisar a performance e aplicar boas prÃ¡ticas:

### AnÃ¡lise de Performance

1. **MÃ©tricas de execuÃ§Ã£o**:
   - Analise o tempo total de execuÃ§Ã£o do pipeline
   - Verifique a taxa de transferÃªncia de dados (MB/s)
   - Identifique possÃ­veis gargalos no processo

2. **OtimizaÃ§Ã£o de recursos**:
   - Ajuste o paralelismo da atividade de cÃ³pia conforme o volume de dados
   - Configure o tamanho apropriado do Integration Runtime
   - Utilize compressÃ£o para reduzir o volume de dados transferidos

### Boas PrÃ¡ticas

1. **SeguranÃ§a**:
   - Utilize Key Vault para armazenar credenciais e segredos
   - Implemente controle de acesso baseado em funÃ§Ãµes (RBAC) para recursos Azure
   - Mantenha o Integration Runtime atualizado com as Ãºltimas correÃ§Ãµes de seguranÃ§a

2. **Monitoramento**:
   - Configure alertas para falhas de pipeline
   - Implemente logs detalhados para diagnÃ³stico
   - Utilize o Azure Monitor para acompanhar mÃ©tricas de longo prazo

3. **Escalabilidade**:
   - Projete a soluÃ§Ã£o considerando o crescimento futuro do volume de dados
   - Utilize particionamento para lidar com grandes conjuntos de dados
   - Considere a implementaÃ§Ã£o de processamento incremental para reduzir a carga

4. **RecuperaÃ§Ã£o**:
   - Implemente mecanismos de retry para lidar com falhas temporÃ¡rias
   - Configure checkpoints para permitir a retomada de transferÃªncias interrompidas
   - Documente procedimentos de recuperaÃ§Ã£o para diferentes cenÃ¡rios de falha

A aplicaÃ§Ã£o dessas prÃ¡ticas garante que o processo de redundÃ¢ncia seja nÃ£o apenas funcional, mas tambÃ©m robusto, seguro e eficiente.

## Prints (DescriÃ§Ãµes Visuais Aprimoradas)

Embora capturas de tela reais nÃ£o possam ser incorporadas diretamente aqui, esta seÃ§Ã£o aprimorada visa fornecer descriÃ§Ãµes textuais ainda mais detalhadas, simulando a experiÃªncia visual de navegar pelo Portal Azure e pelo ADF Studio durante a configuraÃ§Ã£o:

1.  **ConfiguraÃ§Ã£o do SHIR (Azure Portal):**
    *   *DescriÃ§Ã£o Visual:* Imagine a tela "Integration Runtimes" no ADF Studio (aba "Manage"). Uma lista de runtimes existentes Ã© exibida. Clicamos em "+ New". Uma janela lateral surge, mostrando opÃ§Ãµes como "Azure, Self-Hosted" e "Azure-SSIS". Selecionamos "Self-Hosted" e clicamos "Continue". Na prÃ³xima tela, inserimos o nome "OnPremisesIntegrationRuntime" e uma descriÃ§Ã£o opcional. Clicamos em "Create". Uma nova janela pop-up aparece com duas opÃ§Ãµes: "Option 1: Express setup" e "Option 2: Manual setup". Abaixo, vemos as "Authentication key 1" e "Authentication key 2" (ofuscadas, com um botÃ£o para copiar ao lado). Copiamos a Chave 1.

2.  **InstalaÃ§Ã£o do SHIR (Servidor On-Premises):**
    *   *DescriÃ§Ã£o Visual:* Visualizamos o assistente de instalaÃ§Ã£o do "Microsoft Integration Runtime" em uma mÃ¡quina Windows Server. ApÃ³s aceitar os termos, o instalador copia os arquivos. A tela final Ã© o "Integration Runtime Configuration Manager". Um campo de texto proeminente pede a "Authentication key". Colamos a chave copiada do portal Azure. Clicamos em "Register". ApÃ³s alguns instantes, uma marca de verificaÃ§Ã£o verde aparece ao lado de "Registration", e o status do nÃ³ muda para "Running" e "Connected to cloud service".

3.  **CriaÃ§Ã£o do Linked Service (SQL On-Premises):**
    *   *DescriÃ§Ã£o Visual:* No ADF Studio (aba "Manage" > "Linked Services"), clicamos em "+ New". Uma grade de Ã­cones de fontes de dados aparece. Digitamos "SQL Server" na busca e selecionamos o Ã­cone correspondente. Clicamos "Continue". Na tela de configuraÃ§Ã£o, preenchemos:
        *   Name: `SqlServerOnPremises`
        *   Description: (Opcional)
        *   Connect via integration runtime: Selecionamos `OnPremisesIntegrationRuntime` na lista suspensa.
        *   Server name: Digitamos o nome ou IP do servidor SQL local (ex: `SRV-SQL-01`).
        *   Database name: Digitamos o nome do banco (ex: `VendasDB`).
        *   Authentication type: Selecionamos "SQL Authentication".
        *   User name: Digitamos o usuÃ¡rio SQL (ex: `adf_user`).
        *   Password: Selecionamos "Azure Key Vault" (idealmente) ou inserimos a senha diretamente.
    *   Clicamos em "Test connection". ApÃ³s alguns segundos, uma mensagem "Connection successful" aparece em verde. Clicamos em "Create".

4.  **CriaÃ§Ã£o do Dataset (ADLS - TXT):**
    *   *DescriÃ§Ã£o Visual:* No ADF Studio (aba "Author" > "+" > "Dataset"), selecionamos "Azure Data Lake Storage Gen2" e depois "DelimitedText". Clicamos "Continue". Na tela "Set properties":
        *   Name: `AdlsTextFile`
        *   Linked service: Selecionamos `AzureDataLakeStorage`.
        *   File path: Clicamos em "Browse". Navegamos atÃ© o container desejado (ex: `datalake`) e digitamos `raw/` no caminho. Deixamos o nome do arquivo em branco por enquanto (serÃ¡ dinÃ¢mico no pipeline).
        *   First row as header: Marcamos a caixa de seleÃ§Ã£o.
        *   Import schema: Selecionamos "None" (ou "From connection/store" se um arquivo de exemplo existir).
    *   Clicamos em "OK".

5.  **ConfiguraÃ§Ã£o da Atividade de CÃ³pia (Sink):**
    *   *DescriÃ§Ã£o Visual:* No canvas do pipeline "SqlToAdlsRedundancyPipeline", selecionamos a atividade "CopySqlToAdls". Na aba "Sink", com o dataset `AdlsTextFile` selecionado, localizamos o campo "File path" dentro das configuraÃ§Ãµes do dataset. Ao lado do campo do nome do arquivo, clicamos em "Add dynamic content [Alt+P]". Uma janela "Pipeline expression builder" abre. Inserimos a expressÃ£o para o nome dinÃ¢mico, por exemplo: `@{concat(\'dados_vendas_\',utcnow(\'yyyyMMdd_HHmmss\'),\'.txt\')}`. Clicamos em "Finish".

6.  **Monitoramento da ExecuÃ§Ã£o:**
    *   *DescriÃ§Ã£o Visual:* No ADF Studio (aba "Monitor" > "Pipeline runs"), vemos uma lista de execuÃ§Ãµes de pipeline. A linha correspondente Ã  nossa execuÃ§Ã£o manual do "SqlToAdlsRedundancyPipeline" mostra o status "Succeeded" em verde. Clicamos no nome do pipeline. A visualizaÃ§Ã£o detalhada mostra a atividade "CopySqlToAdls" tambÃ©m com status "Succeeded". Clicamos no Ã­cone de Ã³culos ("Details") na linha da atividade. Uma janela pop-up exibe detalhes como "Data read", "Data written", "Files read", "Files written", "Throughput", "Duration", etc.

Estas descriÃ§Ãµes visuais aprimoradas buscam fornecer uma imagem mental mais clara das interfaces e configuraÃ§Ãµes chave encontradas durante a implementaÃ§Ã£o do projeto.

## Insights e Aprendizados Aprofundados

A implementaÃ§Ã£o deste projeto nÃ£o apenas resulta em uma soluÃ§Ã£o tÃ©cnica, mas tambÃ©m gera aprendizados importantes sobre a plataforma Azure e as prÃ¡ticas de engenharia de dados:

*   **Complexidade da IntegraÃ§Ã£o HÃ­brida Gerenciada:** O Self-Hosted Integration Runtime (SHIR) abstrai grande parte da complexidade da conectividade segura entre a nuvem e o ambiente local. No entanto, a *gestÃ£o* do SHIR (atualizaÃ§Ãµes, monitoramento de recursos na mÃ¡quina host, alta disponibilidade com mÃºltiplos nÃ³s) requer atenÃ§Ã£o contÃ­nua. Percebe-se que, embora o ADF facilite a conexÃ£o, a responsabilidade pela infraestrutura do SHIR permanece no lado on-premises, exigindo planejamento de capacidade e manutenÃ§Ã£o.

*   **O Poder da ParametrizaÃ§Ã£o:** A capacidade de usar parÃ¢metros e expressÃµes dinÃ¢micas no ADF (como visto na nomeaÃ§Ã£o dinÃ¢mica de arquivos no Sink) Ã© fundamental para criar pipelines reutilizÃ¡veis e flexÃ­veis. Aprendemos que investir tempo na parametrizaÃ§Ã£o desde o inÃ­cio evita a proliferaÃ§Ã£o de pipelines quase idÃªnticos e facilita a manutenÃ§Ã£o. Por exemplo, poderÃ­amos parametrizar o nome da tabela de origem, o container de destino ou atÃ© mesmo a consulta SQL, tornando o pipeline agnÃ³stico a detalhes especÃ­ficos.

*   **Arquitetura de Camadas na PrÃ¡tica:** Implementar a camada "raw" (ou "bronze") no Data Lake Ã© mais do que apenas um conceito teÃ³rico. Na prÃ¡tica, isso significa configurar o pipeline para despejar os dados *como estÃ£o* (ou com o mÃ­nimo de transformaÃ§Ã£o, como a conversÃ£o para TXT), preservando a fidelidade da origem. Isso desacopla a ingestÃ£o da transformaÃ§Ã£o, permitindo que outras equipes ou processos consumam os dados brutos ou que transformaÃ§Ãµes futuras sejam aplicadas sem re-ingestÃ£o. A escolha do formato (TXT, Parquet, Delta) na camada raw tambÃ©m tem implicaÃ§Ãµes significativas em custo e performance de leitura subsequente.

*   **Monitoramento AlÃ©m do Sucesso/Falha:** A aba "Monitor" do ADF oferece muito mais do que apenas indicar se um pipeline funcionou. A anÃ¡lise dos detalhes da atividade de cÃ³pia (throughput, DIUs/vCore hours consumidos, duraÃ§Ã£o) fornece insights cruciais para otimizaÃ§Ã£o. Aprendemos a correlacionar o volume de dados com o tempo de execuÃ§Ã£o e o custo (implÃ­cito no consumo de DIUs), permitindo identificar se o SHIR estÃ¡ subdimensionado, se a rede Ã© um gargalo, ou se a configuraÃ§Ã£o de paralelismo da cÃ³pia precisa de ajuste.

*   **ImplicaÃ§Ãµes de Custo NÃ£o Ã“bvias:** Embora a execuÃ§Ã£o de pipelines seja um custo direto, a escolha da arquitetura impacta outros custos. Usar SHIR nÃ£o tem custo direto, mas consome recursos da mÃ¡quina host. Armazenar dados no ADLS tem um custo, e o formato (TXT vs. Parquet comprimido) afeta tanto o custo de armazenamento quanto o custo de processamento subsequente. A frequÃªncia de execuÃ§Ã£o dos pipelines tambÃ©m Ã© um fator direto no custo total da soluÃ§Ã£o.

*   **EvoluÃ§Ã£o para Processamento Incremental:** A redundÃ¢ncia total (copiar a tabela inteira a cada execuÃ§Ã£o) Ã© simples, mas ineficiente para tabelas grandes. Um aprendizado chave Ã© a necessidade de evoluir para cargas incrementais. Isso envolveria identificar novas linhas ou linhas modificadas na origem (usando colunas de data/hora de modificaÃ§Ã£o, Change Data Capture - CDC, ou tabelas de controle) e copiar apenas o delta. O ADF suporta padrÃµes para isso, como o uso de "lookup activities" para encontrar o Ãºltimo valor carregado (marca d\'Ã¡gua) e filtrar a consulta de origem.

*   **SeguranÃ§a em Profundidade:** A configuraÃ§Ã£o inicial pode usar autenticaÃ§Ã£o SQL simples, mas aprendemos a importÃ¢ncia de migrar para prÃ¡ticas mais seguras, como o uso do Azure Key Vault para armazenar senhas e strings de conexÃ£o, e, idealmente, usar autenticaÃ§Ã£o baseada em identidade (Managed Identity do ADF) sempre que possÃ­vel para acessar recursos Azure como o ADLS e o SQL Azure, eliminando a necessidade de gerenciar credenciais.

## ConclusÃ£o

Este projeto, feito por mim, demonstrou a implementaÃ§Ã£o de um processo completo de redundÃ¢ncia de arquivos utilizando o Azure Data Factory, conectando ambientes on-premises com a nuvem Azure. A soluÃ§Ã£o criada permite extrair dados de uma tabela SQL Server local, transferi-los para o Azure Data Lake Storage e organizÃ¡-los como arquivos .TXT em uma estrutura de camadas.

A abordagem adotada nÃ£o apenas garante a redundÃ¢ncia dos dados, mas tambÃ©m estabelece uma base sÃ³lida para futuras iniciativas de anÃ¡lise e processamento de dados na nuvem. A utilizaÃ§Ã£o de serviÃ§os gerenciados do Azure reduz a complexidade operacional, permitindo que as organizaÃ§Ãµes foquem no valor dos dados em vez da infraestrutura subjacente.

As habilidades e conhecimentos adquiridos neste projeto sÃ£o diretamente aplicÃ¡veis a diversos cenÃ¡rios de integraÃ§Ã£o de dados, migraÃ§Ã£o para a nuvem e estratÃ©gias de continuidade de negÃ³cios, representando um valioso acrÃ©scimo ao portfÃ³lio de competÃªncias em tecnologias Azure.

## ReferÃªncias

- [DocumentaÃ§Ã£o oficial do Azure Data Factory](https://docs.microsoft.com/azure/data-factory/)
- [Guia de configuraÃ§Ã£o do Self-Hosted Integration Runtime](https://docs.microsoft.com/azure/data-factory/create-self-hosted-integration-runtime)
- [Melhores prÃ¡ticas para o Azure Data Factory](https://docs.microsoft.com/azure/data-factory/data-factory-best-practices)
- [DocumentaÃ§Ã£o do Azure Data Lake Storage Gen2](https://docs.microsoft.com/azure/storage/blobs/data-lake-storage-introduction)
- [PadrÃµes de arquitetura de dados na nuvem](https://docs.microsoft.com/azure/architecture/patterns/)


## ðŸ“‹ DescriÃ§Ã£o

Descreva aqui o conteÃºdo desta seÃ§Ã£o.


## ðŸ“¦ InstalaÃ§Ã£o

Descreva aqui o conteÃºdo desta seÃ§Ã£o.


## ðŸ’» Uso

Descreva aqui o conteÃºdo desta seÃ§Ã£o.


## ðŸ“„ LicenÃ§a

Descreva aqui o conteÃºdo desta seÃ§Ã£o.


---

Activity proposed by **DIO** within my learning program in **Microsoft AI for Tech - Azure Databricks**

# Creating File Redundancy Processes in Microsoft Azure

![Hero Image](images/hero_image.png)

## Introduction

This project documents the implementation of a complete file redundancy process in Microsoft Azure using Azure Data Factory (ADF). The solution connects on-premises environments with Azure cloud resources, allowing for the secure and efficient movement of data between these environments. The main focus is to create an automated flow that extracts data from an on-premises SQL Server table, transfers it to Azure Data Lake Storage (ADLS), and converts it into .TXT files organized in a layered structure (raw/bronze).

Data redundancy is an essential practice to ensure the availability, security, and integrity of information in corporate environments. By implementing this project, you will be creating a solution that not only protects data against loss but also facilitates its use in different contexts and applications in the cloud.

## Project Objective

The main objective of this project is to establish an automated and reliable process for creating data redundancy between on-premises environments and the Azure cloud. Specifically, we aim to:

1. Configure the necessary infrastructure in Azure Data Factory to connect on-premises and cloud environments
2. Implement data extraction from a local SQL Server table
3. Transfer this data to Azure Data Lake Storage
4. Convert the data into .TXT files organized in a layered structure
5. Validate, publish, and execute the data pipeline
6. Analyze performance and apply best practices

At the end of this project, we will have a functional solution that can be adapted for different data redundancy scenarios, contributing to business continuity and disaster recovery strategies.

## Prerequisites

To implement this project, the following components and knowledge are required:

### Azure Resources
- An active Microsoft Azure subscription
- Access to create and manage Azure Data Factory resources
- Permission to create and configure storage resources (Azure Data Lake Storage Gen2)
- Ability to create and manage database resources (Azure SQL Database)

### On-premises Environment
- A local SQL Server with a data table for testing
- Administrative permissions on the server to configure the Integration Runtime
- Network connectivity between the local environment and Azure

### Technical Knowledge
- Basic familiarity with the Azure portal and its services
- Understanding of SQL database concepts
- Notions of ETL (Extract, Transform, Load) and data pipelines

## Solution Architecture

The solution implemented in this project follows the architecture below:

1. **On-premises Environment**:
   - SQL Server with source data table
   - Self-hosted Integration Runtime (SHIR) for secure connectivity with Azure

2. **Microsoft Azure**:
   - Azure Data Factory for data flow orchestration
   - Azure SQL Database for optional cloud data storage
   - Azure Data Lake Storage Gen2 for storing .TXT files

3. **Data Flow**:
   - Data extraction from the on-premises SQL Server table
   - Transfer to Azure via Integration Runtime
   - Storage and conversion to .TXT files in Data Lake
   - Organization into layered structure (raw/bronze)

This architecture provides a secure and efficient data flow between the local environment and the cloud, ensuring data redundancy and availability.

## Step-by-Step Implementation

### 1. Self-Hosted Integration Runtime (SHIR) Configuration

The Self-Hosted Integration Runtime is an essential component for connecting Azure Data Factory to resources in private networks, such as on-premises SQL servers. It acts as a secure bridge between environments, allowing data movement without directly exposing local servers to the internet.

To configure SHIR:

1. **Create the Integration Runtime in Azure Data Factory**:
   - In the Azure portal, access your Azure Data Factory instance
   - Navigate to "Manage" > "Integration Runtimes" > "New"
   - Select "Self-Hosted" as the Integration Runtime type
   - Provide a descriptive name (e.g., "OnPremisesIntegrationRuntime")
   - Copy the generated authentication key

2. **Install the Integration Runtime on the on-premises server**:
   - Download the Integration Runtime installer on the server that has access to SQL Server
   - Run the installer and follow the setup wizard
   - When prompted, enter the authentication key copied earlier
   - Wait for registration to complete and confirmation that the Integration Runtime is online

3. **Verify connectivity**:
   - In the Azure portal, confirm that the Integration Runtime status is "Running"
   - Test connectivity to the local SQL Server using the diagnostic option

SHIR is now configured and ready to facilitate secure data transfer between the on-premises environment and Azure.

### 2. Linked Services Creation

Linked Services in Azure Data Factory function as connections to data sources and destinations. In this project, we need to configure three main Linked Services:

1. **Linked Service for On-premises SQL Server**:
   - In ADF Studio, navigate to "Manage" > "Linked Services" > "New"
   - Select "SQL Server" as the type
   - Configure the following parameters:
     - Name: "SqlServerOnPremises"
     - Integration Runtime: select the previously created SHIR
     - Server name: local SQL server address
     - Database name: database containing the source table
     - Authentication type: Windows or SQL Authentication
     - Credentials: user and password with appropriate permissions
   - Test the connection before saving

2. **Linked Service for Azure SQL Database (optional)**:
   - Select "Azure SQL Database" as the type
   - Configure the following parameters:
     - Name: "AzureSqlDatabase"
     - Selection method: "From Azure subscription"
     - Azure subscription: select your subscription
     - Server name: select or create an Azure SQL server
     - Database name: select or create a database
     - Authentication type: SQL Authentication
     - Credentials: Azure database user and password
   - Test the connection before saving

3. **Linked Service for Azure Data Lake Storage Gen2**:
   - Select "Azure Data Lake Storage Gen2" as the type
   - Configure the following parameters:
     - Name: "AzureDataLakeStorage"
     - Selection method: "From Azure subscription"
     - Azure subscription: select your subscription
     - Storage account name: select or create an ADLS Gen2 account
     - Test the connection before saving

These Linked Services establish the necessary connections for data flow between the different sources and destinations of our pipeline.

### 3. Datasets Creation

Datasets in Azure Data Factory represent data structures within data sources. For our project, we need to create the following datasets:

1. **Dataset for On-premises SQL Server table**:
   - In ADF Studio, navigate to "Author" > "+" > "Dataset"
   - Select "SQL Server" as the type
   - Configure the following parameters:
     - Name: "SqlServerTable"
     - Linked Service: select the "SqlServerOnPremises" Linked Service
     - Table name: select the source table (e.g., "dbo.Customers")
   - Preview data to confirm correct access

2. **Dataset for Azure Data Lake Storage (.TXT format)**:
   - Select "DelimitedText" as the type
   - Configure the following parameters:
     - Name: "AdlsTextFile"
     - Linked Service: select the "AzureDataLakeStorage" Linked Service
     - File path: specify the path in the format "container/raw/data.txt"
     - First row as header: "True"
     - Delimiter: select the appropriate delimiter (e.g., comma)
   - Define the schema as needed

These datasets define the data structures that will be used in the copy pipeline, specifying both the source (SQL table) and the destination (.TXT file in Data Lake).

### 4. Copy Pipeline Creation

The Pipeline is the central component of Azure Data Factory, orchestrating data flow between source and destination. For our redundancy project, we will create a pipeline with a copy activity:

1. **Create a new Pipeline**:
   - In ADF Studio, navigate to "Author" > "+" > "Pipeline"
   - Name the pipeline "SqlToAdlsRedundancyPipeline"

2. **Add copy activity**:
   - From the toolbox, drag the "Copy data" activity to the canvas
   - Name the activity "CopySqlToAdls"

3. **Configure source**:
   - In the "Source" tab, select the "SqlServerTable" dataset
   - If necessary, configure a custom SQL query to filter data

4. **Configure destination**:
   - In the "Sink" tab, select the "AdlsTextFile" dataset
   - Configure write options:
     - Copy option: "Add dynamic content" to define the dynamic path
     - File name format: include date/time for versioning (e.g., "data_{yyyy-MM-dd_HH-mm}.txt")

5. **Configure mappings**:
   - In the "Mapping" tab, verify that source columns are correctly mapped to the destination
   - Adjust data types as needed

6. **Configure additional settings**:
   - In the "Settings" tab, configure:
     - Fault tolerance: define behavior in case of errors
     - Parallelism: adjust according to data volume needs
     - Activity log: enable for detailed monitoring

7. **Add dynamic parameters (optional)**:
   - Configure pipeline parameters to make the solution more flexible
   - Use expressions to define dynamic paths based on date/time

The pipeline is now configured to extract data from the on-premises SQL Server table and transfer it to Azure Data Lake Storage as .TXT files, following the defined layered structure.

### 5. Validation, Publishing, and Execution

After configuring all components, it's time to validate, publish, and execute the pipeline:

1. **Validate the pipeline**:
   - Click the "Validate" button to check for configuration errors
   - Correct any identified issues

2. **Publish changes**:
   - Click the "Publish all" button to publish all components (Linked Services, Datasets, Pipeline)
   - Confirm publication and wait for completion

3. **Execute the pipeline**:
   - Click "Add trigger" > "Trigger now" to manually execute the pipeline
   - Alternatively, configure a recurring trigger:
     - "Add trigger" > "New/Edit" > configure the desired schedule (e.g., daily at 2 AM)

4. **Monitor execution**:
   - Navigate to the "Monitor" tab to track execution progress
   - Check status, duration, and details of each activity
   - In case of failures, analyze error logs to identify and correct problems

5. **Verify results**:
   - Access Azure Data Lake Storage to confirm that .TXT files were created correctly
   - Verify that the folder structure (raw/bronze) is as expected
   - Open some files to confirm that data was transferred correctly

Successful pipeline execution confirms that the redundancy process is working correctly, transferring data from the on-premises environment to Azure securely and organized.

## Performance Analysis and Best Practices

To ensure that the redundancy process is efficient and scalable, it is important to analyze performance and apply best practices:

### Performance Analysis

1. **Execution metrics**:
   - Analyze the total pipeline execution time
   - Check data transfer rate (MB/s)
   - Identify potential bottlenecks in the process

2. **Resource optimization**:
   - Adjust copy activity parallelism according to data volume
   - Configure appropriate Integration Runtime size
   - Use compression to reduce the volume of transferred data

### Best Practices

1. **Security**:
   - Use Key Vault to store credentials and secrets
   - Implement role-based access control (RBAC) for Azure resources
   - Keep the Integration Runtime updated with the latest security patches

2. **Monitoring**:
   - Configure alerts for pipeline failures
   - Implement detailed logs for diagnosis
   - Use Azure Monitor to track long-term metrics

3. **Scalability**:
   - Design the solution considering future data volume growth
   - Use partitioning to handle large datasets
   - Consider implementing incremental processing to reduce load

4. **Recovery**:
   - Implement retry mechanisms to handle temporary failures
   - Configure checkpoints to allow resumption of interrupted transfers
   - Document recovery procedures for different failure scenarios

Applying these practices ensures that the redundancy process is not only functional but also robust, secure, and efficient.

## Prints (Enhanced Visual Descriptions)

Although actual screenshots cannot be directly incorporated here, this enhanced section aims to provide even more detailed textual descriptions, simulating the visual experience of navigating the Azure Portal and ADF Studio during configuration:

1.  **SHIR Configuration (Azure Portal):**
    *   *Visual Description:* Imagine the "Integration Runtimes" screen in ADF Studio ("Manage" tab). A list of existing runtimes is displayed. We click on "+ New". A side window appears, showing options like "Azure, Self-Hosted" and "Azure-SSIS". We select "Self-Hosted" and click "Continue". On the next screen, we enter the name "OnPremisesIntegrationRuntime" and an optional description. We click "Create". A new pop-up window appears with two options: "Option 1: Express setup" and "Option 2: Manual setup". Below, we see "Authentication key 1" and "Authentication key 2" (obfuscated, with a copy button next to them). We copy Key 1.

2.  **SHIR Installation (On-Premises Server):**
    *   *Visual Description:* We visualize the "Microsoft Integration Runtime" installation wizard on a Windows Server machine. After accepting the terms, the installer copies the files. The final screen is the "Integration Runtime Configuration Manager". A prominent text field asks for the "Authentication key". We paste the key copied from the Azure portal. We click "Register". After a few moments, a green checkmark appears next to "Registration", and the node status changes to "Running" and "Connected to cloud service".

3.  **Linked Service Creation (On-Premises SQL):**
    *   *Visual Description:* In ADF Studio ("Manage" > "Linked Services" tab), we click on "+ New". A grid of data source icons appears. We type "SQL Server" in the search and select the corresponding icon. We click "Continue". On the configuration screen, we fill in:
        *   Name: `SqlServerOnPremises`
        *   Description: (Optional)
        *   Connect via integration runtime: We select `OnPremisesIntegrationRuntime` from the dropdown list.
        *   Server name: We type the name or IP of the local SQL server (e.g., `SRV-SQL-01`).
        *   Database name: We type the database name (e.g., `SalesDB`).
        *   Authentication type: We select "SQL Authentication".
        *   User name: We type the SQL user (e.g., `adf_user`).
        *   Password: We select "Azure Key Vault" (ideally) or enter the password directly.
    *   We click "Test connection". After a few seconds, a "Connection successful" message appears in green. We click "Create".

4.  **Dataset Creation (ADLS - TXT):**
    *   *Visual Description:* In ADF Studio ("Author" > "+" > "Dataset" tab), we select "Azure Data Lake Storage Gen2" and then "DelimitedText". We click "Continue". On the "Set properties" screen:
        *   Name: `AdlsTextFile`
        *   Linked service: We select `AzureDataLakeStorage`.
        *   File path: We click "Browse". We navigate to the desired container (e.g., `datalake`) and type `raw/` in the path. We leave the file name blank for now (it will be dynamic in the pipeline).
        *   First row as header: We check the checkbox.
        *   Import schema: We select "None" (or "From connection/store" if an example file exists).
    *   We click "OK".

5.  **Copy Activity Configuration (Sink):**
    *   *Visual Description:* On the "SqlToAdlsRedundancyPipeline" pipeline canvas, we select the "CopySqlToAdls" activity. In the "Sink" tab, with the `AdlsTextFile` dataset selected, we locate the "File path" field within the dataset settings. Next to the file name field, we click "Add dynamic content [Alt+P]". A "Pipeline expression builder" window opens. We enter the expression for the dynamic name, for example: `@{concat(\'sales_data_\',utcnow(\'yyyyMMdd_HHmmss\'),\'.txt\')}`. We click "Finish".

6.  **Execution Monitoring:**
    *   *Visual Description:* In ADF Studio ("Monitor" > "Pipeline runs" tab), we see a list of pipeline runs. The row corresponding to our manual execution of "SqlToAdlsRedundancyPipeline" shows the status "Succeeded" in green. We click on the pipeline name. The detailed view shows the "CopySqlToAdls" activity also with "Succeeded" status. We click the glasses icon ("Details") on the activity row. A pop-up window displays details such as "Data read", "Data written", "Files read", "Files written", "Throughput", "Duration", etc.

These enhanced visual descriptions aim to provide a clearer mental image of the key interfaces and configurations encountered during the project implementation.

## In-depth Insights and Learnings

Implementing this project not only results in a technical solution but also generates important learnings about the Azure platform and data engineering practices:

*   **Complexity of Managed Hybrid Integration:** The Self-Hosted Integration Runtime (SHIR) abstracts much of the complexity of secure connectivity between the cloud and the on-premises environment. However, *managing* SHIR (updates, monitoring host machine resources, high availability with multiple nodes) requires continuous attention. It is clear that while ADF facilitates the connection, responsibility for SHIR infrastructure remains on the on-premises side, requiring capacity planning and maintenance.

*   **The Power of Parameterization:** The ability to use parameters and dynamic expressions in ADF (as seen in dynamic file naming in the Sink) is fundamental for creating reusable and flexible pipelines. We learned that investing time in parameterization from the outset avoids the proliferation of almost identical pipelines and facilitates maintenance. For example, we could parameterize the source table name, the destination container, or even the SQL query, making the pipeline agnostic to specific details.

*   **Layered Architecture in Practice:** Implementing the "raw" (or "bronze") layer in Data Lake is more than just a theoretical concept. In practice, this means configuring the pipeline to dump data *as is* (or with minimal transformation, such as conversion to TXT), preserving source fidelity. This decouples ingestion from transformation, allowing other teams or processes to consume raw data or for future transformations to be applied without re-ingestion. The choice of format (TXT, Parquet, Delta) in the raw layer also has significant implications for cost and subsequent read performance.

*   **Monitoring Beyond Success/Failure:** The ADF "Monitor" tab offers much more than just indicating whether a pipeline worked. Analyzing the details of the copy activity (throughput, DIUs/vCore hours consumed, duration) provides crucial insights for optimization. We learned to correlate data volume with execution time and cost (implicit in DIU consumption), allowing us to identify if SHIR is undersized, if the network is a bottleneck, or if the copy parallelism configuration needs adjustment.

*   **Non-Obvious Cost Implications:** While pipeline execution is a direct cost, architectural choices impact other costs. Using SHIR has no direct cost but consumes host machine resources. Storing data in ADLS has a cost, and the format (TXT vs. compressed Parquet) affects both storage cost and subsequent processing cost. Pipeline execution frequency is also a direct factor in the total solution cost.

*   **Evolution to Incremental Processing:** Full redundancy (copying the entire table with each execution) is simple but inefficient for large tables. A key learning is the need to evolve to incremental loads. This would involve identifying new or modified rows at the source (using modification date/time columns, Change Data Capture - CDC, or control tables) and copying only the delta. ADF supports patterns for this, such as using "lookup activities" to find the last loaded value (watermark) and filter the source query.

*   **Security in Depth:** Initial configuration may use simple SQL authentication, but we learned the importance of migrating to more secure practices, such as using Azure Key Vault to store passwords and connection strings, and, ideally, using identity-based authentication (ADF Managed Identity) whenever possible to access Azure resources like ADLS and Azure SQL, eliminating the need to manage credentials.

## Conclusion

This project, done by me, demonstrated the implementation of a complete file redundancy process using Azure Data Factory, connecting on-premises environments with the Azure cloud. The created solution allows extracting data from a local SQL Server table, transferring it to Azure Data Lake Storage, and organizing it as .TXT files in a layered structure.

The adopted approach not only ensures data redundancy but also establishes a solid foundation for future data analysis and processing initiatives in the cloud. The use of managed Azure services reduces operational complexity, allowing organizations to focus on the value of data instead of the underlying infrastructure.

The skills and knowledge acquired in this project are directly applicable to various data integration scenarios, cloud migration, and business continuity strategies, representing a valuable addition to the portfolio of competencies in Azure technologies.

## References

- [Official Azure Data Factory documentation](https://docs.microsoft.com/azure/data-factory/)
- [Self-Hosted Integration Runtime configuration guide](https://docs.microsoft.com/azure/data-factory/create-self-hosted-integration-runtime)
- [Best practices for Azure Data Factory](https://docs.microsoft.com/azure/data-factory/data-factory-best-practices)
- [Azure Data Lake Storage Gen2 documentation](https://docs.microsoft.com/azure/storage/blobs/data-lake-storage-introduction)
- [Cloud data architecture patterns](https://docs.microsoft.com/azure/architecture/patterns/)


## ðŸ“‹ Description

Describe the content of this section here.


## ðŸ“¦ Installation

Describe the content of this section here.


## ðŸ’» Usage

Describe the content of this section here.


## ðŸ“„ License

Describe the content of this section here.

