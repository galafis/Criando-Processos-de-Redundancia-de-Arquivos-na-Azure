Atividade proposta pela **DIO** dentro do meu programa de aprendizado em **Microsoft AI for Tech - Azure Databricks**

# Criando Processos de Redund√¢ncia de Arquivos no Microsoft Azure

## Introdu√ß√£o

Este projeto documenta a implementa√ß√£o de um processo completo de redund√¢ncia de arquivos no Microsoft Azure utilizando o Azure Data Factory (ADF). A solu√ß√£o conecta ambientes on-premises com recursos na nuvem Azure, permitindo a movimenta√ß√£o segura e eficiente de dados entre esses ambientes. O foco principal √© criar um fluxo automatizado que extrai dados de uma tabela SQL Server on-premises, transfere-os para o Azure Data Lake Storage (ADLS) e os converte em arquivos .TXT organizados em uma estrutura de camadas (raw/bronze).

A redund√¢ncia de dados √© uma pr√°tica essencial para garantir a disponibilidade, a seguran√ßa e a integridade das informa√ß√µes em ambientes corporativos. Ao implementar este projeto, voc√™ estar√° criando uma solu√ß√£o que n√£o apenas protege os dados contra perdas, mas tamb√©m facilita sua utiliza√ß√£o em diferentes contextos e aplica√ß√µes na nuvem.

## Objetivo do Projeto

O objetivo principal deste projeto √© estabelecer um processo automatizado e confi√°vel para criar redund√¢ncia de dados entre ambientes on-premises e a nuvem Azure. Especificamente, buscamos:

1. Configurar a infraestrutura necess√°ria no Azure Data Factory para conectar ambientes on-premises e na nuvem
2. Implementar a extra√ß√£o de dados de uma tabela SQL Server local
3. Transferir esses dados para o Azure Data Lake Storage
4. Converter os dados em arquivos .TXT organizados em uma estrutura de camadas
5. Validar, publicar e executar o pipeline de dados
6. Analisar a performance e aplicar boas pr√°ticas

Ao final deste projeto, teremos uma solu√ß√£o funcional que pode ser adaptada para diferentes cen√°rios de redund√¢ncia de dados, contribuindo para estrat√©gias de continuidade de neg√≥cios e recupera√ß√£o de desastres.

## Pr√©-requisitos

Para implementar este projeto, s√£o necess√°rios os seguintes componentes e conhecimentos:

### Recursos Azure
- Uma assinatura ativa do Microsoft Azure
- Acesso para criar e gerenciar recursos do Azure Data Factory
- Permiss√£o para criar e configurar recursos de armazenamento (Azure Data Lake Storage Gen2)
- Capacidade de criar e gerenciar recursos de banco de dados (Azure SQL Database)

### Ambiente On-premises
- Um servidor SQL Server local com uma tabela de dados para teste
- Permiss√µes administrativas no servidor para configurar o Integration Runtime
- Conectividade de rede entre o ambiente local e o Azure

### Conhecimentos T√©cnicos
- Familiaridade b√°sica com o portal Azure e seus servi√ßos
- Entendimento de conceitos de bancos de dados SQL
- No√ß√µes de ETL (Extract, Transform, Load) e pipelines de dados

## Arquitetura da Solu√ß√£o

A solu√ß√£o implementada neste projeto segue a seguinte arquitetura:

1. **Ambiente On-premises**:
   - SQL Server com tabela de dados de origem
   - Self-hosted Integration Runtime (SHIR) para conectividade segura com o Azure

2. **Microsoft Azure**:
   - Azure Data Factory para orquestra√ß√£o do fluxo de dados
   - Azure SQL Database para armazenamento opcional de dados na nuvem
   - Azure Data Lake Storage Gen2 para armazenamento dos arquivos .TXT

3. **Fluxo de Dados**:
   - Extra√ß√£o de dados da tabela SQL Server on-premises
   - Transfer√™ncia para o Azure via Integration Runtime
   - Armazenamento e convers√£o para arquivos .TXT no Data Lake
   - Organiza√ß√£o em estrutura de camadas (raw/bronze)

Esta arquitetura proporciona um fluxo seguro e eficiente de dados entre o ambiente local e a nuvem, garantindo a redund√¢ncia e a disponibilidade das informa√ß√µes.

## Passo a Passo da Implementa√ß√£o

### 1. Configura√ß√£o do Self-Hosted Integration Runtime (SHIR)

O Self-Hosted Integration Runtime √© um componente essencial para conectar o Azure Data Factory a recursos em redes privadas, como servidores SQL on-premises. Ele atua como uma ponte segura entre os ambientes, permitindo a movimenta√ß√£o de dados sem expor diretamente os servidores locais √† internet.

Para configurar o SHIR:

1. **Criar o Integration Runtime no Azure Data Factory**:
   - No portal Azure, acesse sua inst√¢ncia do Azure Data Factory
   - Navegue at√© "Manage" > "Integration Runtimes" > "New"
   - Selecione "Self-Hosted" como tipo de Integration Runtime
   - Forne√ßa um nome descritivo (ex: "OnPremisesIntegrationRuntime")
   - Copie a chave de autentica√ß√£o gerada

2. **Instalar o Integration Runtime no servidor on-premises**:
   - Baixe o instalador do Integration Runtime no servidor que tem acesso ao SQL Server
   - Execute o instalador e siga o assistente de configura√ß√£o
   - Quando solicitado, insira a chave de autentica√ß√£o copiada anteriormente
   - Aguarde a conclus√£o do registro e a confirma√ß√£o de que o Integration Runtime est√° online

3. **Verificar a conectividade**:
   - No portal Azure, confirme que o status do Integration Runtime est√° como "Running"
   - Teste a conectividade com o SQL Server local usando a op√ß√£o de diagn√≥stico

O SHIR agora est√° configurado e pronto para facilitar a transfer√™ncia segura de dados entre o ambiente on-premises e o Azure.

### 2. Cria√ß√£o de Linked Services

Os Linked Services no Azure Data Factory funcionam como conex√µes para fontes de dados e destinos. Neste projeto, precisamos configurar tr√™s Linked Services principais:

1. **Linked Service para SQL Server On-premises**:
   - No ADF Studio, navegue at√© "Manage" > "Linked Services" > "New"
   - Selecione "SQL Server" como tipo
   - Configure os seguintes par√¢metros:
     - Nome: "SqlServerOnPremises"
     - Integration Runtime: selecione o SHIR criado anteriormente
     - Nome do servidor: endere√ßo do servidor SQL local
     - Nome do banco de dados: banco que cont√©m a tabela de origem
     - Tipo de autentica√ß√£o: Windows ou SQL Authentication
     - Credenciais: usu√°rio e senha com permiss√µes adequadas
   - Teste a conex√£o antes de salvar

2. **Linked Service para Azure SQL Database (opcional)**:
   - Selecione "Azure SQL Database" como tipo
   - Configure os seguintes par√¢metros:
     - Nome: "AzureSqlDatabase"
     - M√©todo de sele√ß√£o: "From Azure subscription"
     - Assinatura Azure: selecione sua assinatura
     - Nome do servidor: selecione ou crie um servidor SQL Azure
     - Nome do banco de dados: selecione ou crie um banco de dados
     - Tipo de autentica√ß√£o: SQL Authentication
     - Credenciais: usu√°rio e senha do banco Azure
   - Teste a conex√£o antes de salvar

3. **Linked Service para Azure Data Lake Storage Gen2**:
   - Selecione "Azure Data Lake Storage Gen2" como tipo
   - Configure os seguintes par√¢metros:
     - Nome: "AzureDataLakeStorage"
     - M√©todo de sele√ß√£o: "From Azure subscription"
     - Assinatura Azure: selecione sua assinatura
     - Nome da conta de armazenamento: selecione ou crie uma conta ADLS Gen2
     - Teste a conex√£o antes de salvar

Estes Linked Services estabelecem as conex√µes necess√°rias para o fluxo de dados entre as diferentes fontes e destinos do nosso pipeline.

### 3. Cria√ß√£o de Datasets

Os Datasets no Azure Data Factory representam estruturas de dados dentro das fontes de dados. Para nosso projeto, precisamos criar os seguintes datasets:

1. **Dataset para tabela SQL Server On-premises**:
   - No ADF Studio, navegue at√© "Author" > "+" > "Dataset"
   - Selecione "SQL Server" como tipo
   - Configure os seguintes par√¢metros:
     - Nome: "SqlServerTable"
     - Linked Service: selecione o Linked Service "SqlServerOnPremises"
     - Nome da tabela: selecione a tabela de origem (ex: "dbo.Customers")
   - Visualize os dados para confirmar o acesso correto

2. **Dataset para Azure Data Lake Storage (formato .TXT)**:
   - Selecione "DelimitedText" como tipo
   - Configure os seguintes par√¢metros:
     - Nome: "AdlsTextFile"
     - Linked Service: selecione o Linked Service "AzureDataLakeStorage"
     - Caminho do arquivo: especifique o caminho no formato "container/raw/data.txt"
     - Primeira linha como cabe√ßalho: "True"
     - Delimitador: selecione o delimitador apropriado (ex: v√≠rgula)
   - Defina o esquema conforme necess√°rio

Estes datasets definem as estruturas de dados que ser√£o utilizadas no pipeline de c√≥pia, especificando tanto a origem (tabela SQL) quanto o destino (arquivo .TXT no Data Lake).

### 4. Cria√ß√£o do Pipeline de C√≥pia

O Pipeline √© o componente central do Azure Data Factory, orquestrando o fluxo de dados entre a origem e o destino. Para nosso projeto de redund√¢ncia, vamos criar um pipeline com uma atividade de c√≥pia:

1. **Criar um novo Pipeline**:
   - No ADF Studio, navegue at√© "Author" > "+" > "Pipeline"
   - Nomeie o pipeline como "SqlToAdlsRedundancyPipeline"

2. **Adicionar atividade de c√≥pia**:
   - Na caixa de ferramentas, arraste a atividade "Copy data" para o canvas
   - Nomeie a atividade como "CopySqlToAdls"

3. **Configurar a origem**:
   - Na aba "Source", selecione o dataset "SqlServerTable"
   - Se necess√°rio, configure uma consulta SQL personalizada para filtrar os dados

4. **Configurar o destino**:
   - Na aba "Sink", selecione o dataset "AdlsTextFile"
   - Configure as op√ß√µes de escrita:
     - Op√ß√£o de c√≥pia: "Add dynamic content" para definir o caminho din√¢mico
     - Formato de nome de arquivo: inclua data/hora para versionamento (ex: "data_{yyyy-MM-dd_HH-mm}.txt")

5. **Configurar mapeamentos**:
   - Na aba "Mapping", verifique se as colunas da origem est√£o corretamente mapeadas para o destino
   - Ajuste tipos de dados conforme necess√°rio

6. **Configurar configura√ß√µes adicionais**:
   - Na aba "Settings", configure:
     - Toler√¢ncia a falhas: defina comportamento em caso de erros
     - Paralelismo: ajuste conforme necessidade de performance
     - Log de atividades: habilite para monitoramento detalhado

7. **Adicionar par√¢metros din√¢micos (opcional)**:
   - Configure par√¢metros de pipeline para tornar a solu√ß√£o mais flex√≠vel
   - Utilize express√µes para definir caminhos din√¢micos baseados em data/hora

O pipeline agora est√° configurado para extrair dados da tabela SQL Server on-premises e transferi-los para o Azure Data Lake Storage como arquivos .TXT, seguindo a estrutura de camadas definida.

### 5. Valida√ß√£o, Publica√ß√£o e Execu√ß√£o

Ap√≥s configurar todos os componentes, √© hora de validar, publicar e executar o pipeline:

1. **Validar o pipeline**:
   - Clique no bot√£o "Validate" para verificar se h√° erros de configura√ß√£o
   - Corrija quaisquer problemas identificados

2. **Publicar as altera√ß√µes**:
   - Clique no bot√£o "Publish all" para publicar todos os componentes (Linked Services, Datasets, Pipeline)
   - Confirme a publica√ß√£o e aguarde a conclus√£o

3. **Executar o pipeline**:
   - Clique em "Add trigger" > "Trigger now" para executar o pipeline manualmente
   - Alternativamente, configure um trigger recorrente:
     - "Add trigger" > "New/Edit" > configure a programa√ß√£o desejada (ex: diariamente √†s 2h)

4. **Monitorar a execu√ß√£o**:
   - Navegue at√© a aba "Monitor" para acompanhar o progresso da execu√ß√£o
   - Verifique o status, dura√ß√£o e detalhes de cada atividade
   - Em caso de falhas, analise os logs de erro para identificar e corrigir problemas

5. **Verificar os resultados**:
   - Acesse o Azure Data Lake Storage para confirmar que os arquivos .TXT foram criados corretamente
   - Verifique se a estrutura de pastas (raw/bronze) est√° conforme o esperado
   - Abra alguns arquivos para confirmar que os dados foram transferidos corretamente

A execu√ß√£o bem-sucedida do pipeline confirma que o processo de redund√¢ncia est√° funcionando corretamente, transferindo dados do ambiente on-premises para o Azure de forma segura e organizada.

## An√°lise de Performance e Boas Pr√°ticas

Para garantir que o processo de redund√¢ncia seja eficiente e escal√°vel, √© importante analisar a performance e aplicar boas pr√°ticas:

### An√°lise de Performance

1. **M√©tricas de execu√ß√£o**:
   - Analise o tempo total de execu√ß√£o do pipeline
   - Verifique a taxa de transfer√™ncia de dados (MB/s)
   - Identifique poss√≠veis gargalos no processo

2. **Otimiza√ß√£o de recursos**:
   - Ajuste o paralelismo da atividade de c√≥pia conforme o volume de dados
   - Configure o tamanho apropriado do Integration Runtime
   - Utilize compress√£o para reduzir o volume de dados transferidos

### Boas Pr√°ticas

1. **Seguran√ßa**:
   - Utilize Key Vault para armazenar credenciais e segredos
   - Implemente controle de acesso baseado em fun√ß√µes (RBAC) para recursos Azure
   - Mantenha o Integration Runtime atualizado com as √∫ltimas corre√ß√µes de seguran√ßa

2. **Monitoramento**:
   - Configure alertas para falhas de pipeline
   - Implemente logs detalhados para diagn√≥stico
   - Utilize o Azure Monitor para acompanhar m√©tricas de longo prazo

3. **Escalabilidade**:
   - Projete a solu√ß√£o considerando o crescimento futuro do volume de dados
   - Utilize particionamento para lidar com grandes conjuntos de dados
   - Considere a implementa√ß√£o de processamento incremental para reduzir a carga

4. **Recupera√ß√£o**:
   - Implemente mecanismos de retry para lidar com falhas tempor√°rias
   - Configure checkpoints para permitir a retomada de transfer√™ncias interrompidas
   - Documente procedimentos de recupera√ß√£o para diferentes cen√°rios de falha

A aplica√ß√£o dessas pr√°ticas garante que o processo de redund√¢ncia seja n√£o apenas funcional, mas tamb√©m robusto, seguro e eficiente.

## Prints (Descri√ß√µes Visuais Aprimoradas)

Embora capturas de tela reais n√£o possam ser incorporadas diretamente aqui, esta se√ß√£o aprimorada visa fornecer descri√ß√µes textuais ainda mais detalhadas, simulando a experi√™ncia visual de navegar pelo Portal Azure e pelo ADF Studio durante a configura√ß√£o:

1.  **Configura√ß√£o do SHIR (Azure Portal):**
    *   *Descri√ß√£o Visual:* Imagine a tela "Integration Runtimes" no ADF Studio (aba "Manage"). Uma lista de runtimes existentes √© exibida. Clicamos em "+ New". Uma janela lateral surge, mostrando op√ß√µes como "Azure, Self-Hosted" e "Azure-SSIS". Selecionamos "Self-Hosted" e clicamos "Continue". Na pr√≥xima tela, inserimos o nome "OnPremisesIntegrationRuntime" e uma descri√ß√£o opcional. Clicamos em "Create". Uma nova janela pop-up aparece com duas op√ß√µes: "Option 1: Express setup" e "Option 2: Manual setup". Abaixo, vemos as "Authentication key 1" e "Authentication key 2" (ofuscadas, com um bot√£o para copiar ao lado). Copiamos a Chave 1.

2.  **Instala√ß√£o do SHIR (Servidor On-Premises):**
    *   *Descri√ß√£o Visual:* Visualizamos o assistente de instala√ß√£o do "Microsoft Integration Runtime" em uma m√°quina Windows Server. Ap√≥s aceitar os termos, o instalador copia os arquivos. A tela final √© o "Integration Runtime Configuration Manager". Um campo de texto proeminente pede a "Authentication key". Colamos a chave copiada do portal Azure. Clicamos em "Register". Ap√≥s alguns instantes, uma marca de verifica√ß√£o verde aparece ao lado de "Registration", e o status do n√≥ muda para "Running" e "Connected to cloud service".

3.  **Cria√ß√£o do Linked Service (SQL On-Premises):**
    *   *Descri√ß√£o Visual:* No ADF Studio (aba "Manage" > "Linked Services"), clicamos em "+ New". Uma grade de √≠cones de fontes de dados aparece. Digitamos "SQL Server" na busca e selecionamos o √≠cone correspondente. Clicamos "Continue". Na tela de configura√ß√£o, preenchemos:
        *   Name: `SqlServerOnPremises`
        *   Description: (Opcional)
        *   Connect via integration runtime: Selecionamos `OnPremisesIntegrationRuntime` na lista suspensa.
        *   Server name: Digitamos o nome ou IP do servidor SQL local (ex: `SRV-SQL-01`).
        *   Database name: Digitamos o nome do banco (ex: `VendasDB`).
        *   Authentication type: Selecionamos "SQL Authentication".
        *   User name: Digitamos o usu√°rio SQL (ex: `adf_user`).
        *   Password: Selecionamos "Azure Key Vault" (idealmente) ou inserimos a senha diretamente.
    *   Clicamos em "Test connection". Ap√≥s alguns segundos, uma mensagem "Connection successful" aparece em verde. Clicamos em "Create".

4.  **Cria√ß√£o do Dataset (ADLS - TXT):**
    *   *Descri√ß√£o Visual:* No ADF Studio (aba "Author" > "+" > "Dataset"), selecionamos "Azure Data Lake Storage Gen2" e depois "DelimitedText". Clicamos "Continue". Na tela "Set properties":
        *   Name: `AdlsTextFile`
        *   Linked service: Selecionamos `AzureDataLakeStorage`.
        *   File path: Clicamos em "Browse". Navegamos at√© o container desejado (ex: `datalake`) e digitamos `raw/` no caminho. Deixamos o nome do arquivo em branco por enquanto (ser√° din√¢mico no pipeline).
        *   First row as header: Marcamos a caixa de sele√ß√£o.
        *   Import schema: Selecionamos "None" (ou "From connection/store" se um arquivo de exemplo existir).
    *   Clicamos em "OK".

5.  **Configura√ß√£o da Atividade de C√≥pia (Sink):**
    *   *Descri√ß√£o Visual:* No canvas do pipeline "SqlToAdlsRedundancyPipeline", selecionamos a atividade "CopySqlToAdls". Na aba "Sink", com o dataset `AdlsTextFile` selecionado, localizamos o campo "File path" dentro das configura√ß√µes do dataset. Ao lado do campo do nome do arquivo, clicamos em "Add dynamic content [Alt+P]". Uma janela "Pipeline expression builder" abre. Inserimos a express√£o para o nome din√¢mico, por exemplo: `@{concat('dados_vendas_',utcnow('yyyyMMdd_HHmmss'),'.txt')}`. Clicamos em "Finish".

6.  **Monitoramento da Execu√ß√£o:**
    *   *Descri√ß√£o Visual:* No ADF Studio (aba "Monitor" > "Pipeline runs"), vemos uma lista de execu√ß√µes de pipeline. A linha correspondente √† nossa execu√ß√£o manual do "SqlToAdlsRedundancyPipeline" mostra o status "Succeeded" em verde. Clicamos no nome do pipeline. A visualiza√ß√£o detalhada mostra a atividade "CopySqlToAdls" tamb√©m com status "Succeeded". Clicamos no √≠cone de √≥culos ("Details") na linha da atividade. Uma janela pop-up exibe detalhes como "Data read", "Data written", "Files read", "Files written", "Throughput", "Duration", etc.

Estas descri√ß√µes visuais aprimoradas buscam fornecer uma imagem mental mais clara das interfaces e configura√ß√µes chave encontradas durante a implementa√ß√£o do projeto.

## Insights e Aprendizados Aprofundados

A implementa√ß√£o deste projeto n√£o apenas resulta em uma solu√ß√£o t√©cnica, mas tamb√©m gera aprendizados importantes sobre a plataforma Azure e as pr√°ticas de engenharia de dados:

*   **Complexidade da Integra√ß√£o H√≠brida Gerenciada:** O Self-Hosted Integration Runtime (SHIR) abstrai grande parte da complexidade da conectividade segura entre a nuvem e o ambiente local. No entanto, a *gest√£o* do SHIR (atualiza√ß√µes, monitoramento de recursos na m√°quina host, alta disponibilidade com m√∫ltiplos n√≥s) requer aten√ß√£o cont√≠nua. Percebe-se que, embora o ADF facilite a conex√£o, a responsabilidade pela infraestrutura do SHIR permanece no lado on-premises, exigindo planejamento de capacidade e manuten√ß√£o.

*   **O Poder da Parametriza√ß√£o:** A capacidade de usar par√¢metros e express√µes din√¢micas no ADF (como visto na nomea√ß√£o din√¢mica de arquivos no Sink) √© fundamental para criar pipelines reutiliz√°veis e flex√≠veis. Aprendemos que investir tempo na parametriza√ß√£o desde o in√≠cio evita a prolifera√ß√£o de pipelines quase id√™nticos e facilita a manuten√ß√£o. Por exemplo, poder√≠amos parametrizar o nome da tabela de origem, o container de destino ou at√© mesmo a consulta SQL, tornando o pipeline agn√≥stico a detalhes espec√≠ficos.

*   **Arquitetura de Camadas na Pr√°tica:** Implementar a camada "raw" (ou "bronze") no Data Lake √© mais do que apenas um conceito te√≥rico. Na pr√°tica, isso significa configurar o pipeline para despejar os dados *como est√£o* (ou com o m√≠nimo de transforma√ß√£o, como a convers√£o para TXT), preservando a fidelidade da origem. Isso desacopla a ingest√£o da transforma√ß√£o, permitindo que outras equipes ou processos consumam os dados brutos ou que transforma√ß√µes futuras sejam aplicadas sem re-ingest√£o. A escolha do formato (TXT, Parquet, Delta) na camada raw tamb√©m tem implica√ß√µes significativas em custo e performance de leitura subsequente.

*   **Monitoramento Al√©m do Sucesso/Falha:** A aba "Monitor" do ADF oferece muito mais do que apenas indicar se um pipeline funcionou. A an√°lise dos detalhes da atividade de c√≥pia (throughput, DIUs/vCore hours consumidos, dura√ß√£o) fornece insights cruciais para otimiza√ß√£o. Aprendemos a correlacionar o volume de dados com o tempo de execu√ß√£o e o custo (impl√≠cito no consumo de DIUs), permitindo identificar se o SHIR est√° subdimensionado, se a rede √© um gargalo, ou se a configura√ß√£o de paralelismo da c√≥pia precisa de ajuste.

*   **Implica√ß√µes de Custo N√£o √ìbvias:** Embora a execu√ß√£o de pipelines seja um custo direto, a escolha da arquitetura impacta outros custos. Usar SHIR n√£o tem custo direto, mas consome recursos da m√°quina host. Armazenar dados no ADLS tem um custo, e o formato (TXT vs. Parquet comprimido) afeta tanto o custo de armazenamento quanto o custo de processamento subsequente. A frequ√™ncia de execu√ß√£o dos pipelines tamb√©m √© um fator direto no custo total da solu√ß√£o.

*   **Evolu√ß√£o para Processamento Incremental:** A redund√¢ncia total (copiar a tabela inteira a cada execu√ß√£o) √© simples, mas ineficiente para tabelas grandes. Um aprendizado chave √© a necessidade de evoluir para cargas incrementais. Isso envolveria identificar novas linhas ou linhas modificadas na origem (usando colunas de data/hora de modifica√ß√£o, Change Data Capture - CDC, ou tabelas de controle) e copiar apenas o delta. O ADF suporta padr√µes para isso, como o uso de "lookup activities" para encontrar o √∫ltimo valor carregado (marca d'√°gua) e filtrar a consulta de origem.

*   **Seguran√ßa em Profundidade:** A configura√ß√£o inicial pode usar autentica√ß√£o SQL simples, mas aprendemos a import√¢ncia de migrar para pr√°ticas mais seguras, como o uso do Azure Key Vault para armazenar senhas e strings de conex√£o, e, idealmente, usar autentica√ß√£o baseada em identidade (Managed Identity do ADF) sempre que poss√≠vel para acessar recursos Azure como o ADLS e o SQL Azure, eliminando a necessidade de gerenciar credenciais.

## Conclus√£o

Este projeto, feito por mim, demonstrou a implementa√ß√£o de um processo completo de redund√¢ncia de arquivos utilizando o Azure Data Factory, conectando ambientes on-premises com a nuvem Azure. A solu√ß√£o criada permite extrair dados de uma tabela SQL Server local, transferi-los para o Azure Data Lake Storage e organiz√°-los como arquivos .TXT em uma estrutura de camadas.

A abordagem adotada n√£o apenas garante a redund√¢ncia dos dados, mas tamb√©m estabelece uma base s√≥lida para futuras iniciativas de an√°lise e processamento de dados na nuvem. A utiliza√ß√£o de servi√ßos gerenciados do Azure reduz a complexidade operacional, permitindo que as organiza√ß√µes foquem no valor dos dados em vez da infraestrutura subjacente.

As habilidades e conhecimentos adquiridos neste projeto s√£o diretamente aplic√°veis a diversos cen√°rios de integra√ß√£o de dados, migra√ß√£o para a nuvem e estrat√©gias de continuidade de neg√≥cios, representando um valioso acr√©scimo ao portf√≥lio de compet√™ncias em tecnologias Azure.

## Refer√™ncias

- [Documenta√ß√£o oficial do Azure Data Factory](https://docs.microsoft.com/azure/data-factory/)
- [Guia de configura√ß√£o do Self-Hosted Integration Runtime](https://docs.microsoft.com/azure/data-factory/create-self-hosted-integration-runtime)
- [Melhores pr√°ticas para o Azure Data Factory](https://docs.microsoft.com/azure/data-factory/data-factory-best-practices)
- [Documenta√ß√£o do Azure Data Lake Storage Gen2](https://docs.microsoft.com/azure/storage/blobs/data-lake-storage-introduction)
- [Padr√µes de arquitetura de dados na nuvem](https://docs.microsoft.com/azure/architecture/patterns/)


## üìã Descri√ß√£o

Descreva aqui o conte√∫do desta se√ß√£o.


## üì¶ Instala√ß√£o

Descreva aqui o conte√∫do desta se√ß√£o.


## üíª Uso

Descreva aqui o conte√∫do desta se√ß√£o.


## üìÑ Licen√ßa

Descreva aqui o conte√∫do desta se√ß√£o.
